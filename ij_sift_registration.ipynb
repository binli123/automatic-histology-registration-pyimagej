{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image registration based on SIFT feature matching\n",
    "The algorithm works well when the two images are from the same modality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imagej\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import ast\n",
    "from skimage import io, exposure, img_as_uint, img_as_ubyte, img_as_float\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize PyImageJ\n",
    "This can take a while for the first run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageJ version: 2.3.0/1.53f\n"
     ]
    }
   ],
   "source": [
    "ij = imagej.init('sc.fiji:fiji')\n",
    "print(f\"ImageJ version: {ij.getVersion()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required functions and ImageJ macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'source_image': '',\n",
    "    'target_image': '',\n",
    "    'initial_gaussian_blur': 3.60,\n",
    "    'steps_per_scale_octave': 3,\n",
    "    'minimum_image_size': 56,\n",
    "    'maximum_image_size': 1280, \n",
    "    'feature_descriptor_size': 8, \n",
    "    'feature_descriptor_orientation_bins': 8,\n",
    "    'closest/next_closest_ratio': 0.92,\n",
    "    'filter maximal_alignment_error': 100,\n",
    "    'minimal_inlier_ratio': 0.05,\n",
    "    'minimal_number_of_inliers': 7, \n",
    "    'expected_transformation': 'Similarity'\n",
    "}\n",
    "\n",
    "plugin = \"Extract SIFT Correspondences\"\n",
    "\n",
    "macro_openIm = \"\"\"\n",
    "#@ String path\n",
    "open(path);\n",
    "\"\"\"\n",
    "\n",
    "macro_closeAllIm = \"\"\"\n",
    "close(\"*\");\n",
    "\"\"\"\n",
    "\n",
    "def apply_tform(im, ref, tform, multichannel=False):\n",
    "    if multichannel:\n",
    "        height, width, _ = ref.shape\n",
    "    else:\n",
    "        height, width = ref.shape\n",
    "    transformed_img = cv2.warpPerspective(im.astype(np.uint8),\n",
    "                        tform, (width, height), flags=cv2.INTER_CUBIC)\n",
    "    return transformed_img\n",
    "\n",
    "def find_pop_string(msg, s, e):\n",
    "    check = msg.find(s)\n",
    "    if check == -1:\n",
    "        return -1, msg\n",
    "    found = msg[msg.find(s)+len(s):msg.find(e)+len(e)]\n",
    "    pop = msg[msg.find(e)+len(e):]\n",
    "    return found, pop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running SIFT registration algorithm in FIJI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Register pseudo modality generated by CoMIR. CoMIR convert images of different modality into a common intermediate modality (pseudo_modality.ipynb).\n",
    "R1_ims = glob('CoMIR\\\\output\\\\modalB\\\\*') ### Moving images.\n",
    "R2_ims = glob('CoMIR\\\\output\\\\modalA\\\\*')  ### Fixed images (the order of the file names has to match the moving images' in alphabetical order).\n",
    "source_ims = glob('CoMIR\\\\modalB\\\\*')  ### Source images to be transformed. Usually, these are the moving images.\n",
    "out_path = 'CoMIR\\\\Registered' ### Folder to save transformed images.\n",
    "\n",
    "### Register Brightfield images of tissue captured by different microscope.\n",
    "# R1_ims = glob('BF_set\\\\CAMM\\\\*') ### Moving images.\n",
    "# R2_ims = glob('BF_set\\\\Aperio\\\\*')  ### Fixed images.\n",
    "# source_ims = glob('BF_set\\\\CAMM\\\\*')  \n",
    "# out_path = 'BF_set\\\\Registered' \n",
    "\n",
    "os.makedirs(out_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract SIFT features and compute transform matrix, this can take a while. Output are captured from logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture output\n",
    "R1_ims.sort()\n",
    "R2_ims.sort()\n",
    "source_ims.sort()\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "for R1_im, R2_im, source_im in zip(R1_ims, R2_ims, source_ims):\n",
    "    ij.py.run_macro(macro_closeAllIm)\n",
    "    ij.py.run_macro(macro_openIm, {'path': os.path.join(os.getcwd(), R1_im)})\n",
    "    ij.py.run_macro(macro_openIm, {'path': os.path.join(os.getcwd(), R2_im)})\n",
    "\n",
    "    params['source_image'] = os.path.basename(R1_im)\n",
    "    params['target_image'] = os.path.basename(R2_im)\n",
    "\n",
    "    result = ij.py.run_plugin(plugin, params)\n",
    "    ij.py.run_macro(macro_closeAllIm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[java.lang.Enum.toString] Processing SIFT ...[java.lang.Enum.toString] \n",
      "[java.lang.Enum.toString]  took 1311ms.[java.lang.Enum.toString] \n",
      "[java.lang.Enum.toString] 1368 features extracted.[java.lang.Enum.toString] \n",
      "[java.lang.Enum.toString] Processing SIFT ...[java.lang.Enum.toString] \n",
      "[java.lang.Enum.toString]  took 1153ms.[java.lang.Enum.toString] \n",
      "[java.lang.Enum.toString] 1393 features extracted.[java.lang.Enum.toString] \n",
      "[java.lang.Enum.toString] Identifying correspondence candidates using brute force ...[java.lang.Enum.toString] \n",
      "[java.lang.Enum.toString]  took 981ms.[java.lang.Enum.toString] \n",
      "[java.lang.Enum.toString] 21 potentially corresponding features identified.[java.lang.Enum.toString] \n",
      "[java.lang.Enum.toString] Filtering correspondence candidates by geometric consensus ...[java.lang.Enum.toString] \n",
      "[java.lang.Enum.toString]  took 1ms.[java.lang.Enum.toString] \n",
      "[java.lang.Enum.toString] 10 corresponding features with an average displacement of 19.046px identified.[java.lang.Enum.toString] \n",
      "[java.lang.Enum.toString] Estimated transformation model: [3,3](AffineTransform[[0.983076869761742, 0.012197854396314, -70.74909593064149], [-0.012197854396314, 0.983076869761742, -164.1069176357364]]) 19.046368819801938[java.lang.Enum.toString] \n",
      "[java.lang.Enum.toString] Processing SIFT ...[java.lang.Enum.toString] \n",
      "[java.lang.Enum.toString]  took 1527ms.[java.lang.Enum.toString] \n",
      "[java.lang.Enum.toString] 2030 features extracted.[java.lang.Enum.toString] \n",
      "[java.lang.Enum.toString] Processing SIFT ...[java.lang.Enum.toString] \n",
      "[java.lang.Enum.toString]  took 1711ms.[java.lang.Enum.toString] \n",
      "[java.lang.Enum.toString] 2408 features extracted.[java.lang.Enum.toString] \n",
      "[java.lang.Enum.toString] Identifying correspondence candidates using brute force ...[java.lang.Enum.toString] \n",
      "[java.lang.Enum.toString]  took 2818ms.[java.lang.Enum.toString] \n",
      "[java.lang.Enum.toString] 34 potentially corresponding features identified.[java.lang.Enum.toString] \n",
      "[java.lang.Enum.toString] Filtering correspondence candidates by geometric consensus ...[java.lang.Enum.toString] \n",
      "[java.lang.Enum.toString]  took 1ms.[java.lang.Enum.toString] \n",
      "[java.lang.Enum.toString] 20 corresponding features with an average displacement of 10.456px identified.[java.lang.Enum.toString] \n",
      "[java.lang.Enum.toString] Estimated transformation model: [3,3](AffineTransform[[0.987815074783532, -0.002178175253228, 13.314857382995575], [0.002178175253228, 0.987815074783532, -148.07460332025198]]) 10.456302193678463[java.lang.Enum.toString] \n"
     ]
    }
   ],
   "source": [
    "output.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply transform matrix to source images \n",
    "Scan the output transformation matrix from logs and apply it to the source images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tforms = []\n",
    "searching = True\n",
    "msg = output.stderr\n",
    "tform = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n",
    "for i in range(len(source_ims)):\n",
    "    error_idx = msg.find('No correspondences found.')\n",
    "    tform_idx = msg.find('AffineTransform')\n",
    "    if error_idx < tform_idx and error_idx != -1 or tform_idx == -1:\n",
    "        error, msg = find_pop_string(msg, 'No correspondences found.', 'No correspondences found.')\n",
    "        if error != -1:\n",
    "            print(f'Slide registration failed at {source_ims[i]}, no correspondence found. Use previous tform matrix.')\n",
    "            tforms.append(tform)\n",
    "    elif error_idx == -1 and tform_idx == -1:\n",
    "        print('Process done.')\n",
    "    else:\n",
    "        t_string, msg = find_pop_string(msg, 'AffineTransform', ']]')\n",
    "        # t_string = t_string.replace('E-', '')\n",
    "        tform = ast.literal_eval(t_string)\n",
    "        tform = np.asarray(tform).reshape((2, 3))\n",
    "        tform = np.concatenate((tform, np.array([[0, 0, 1]])), 0)\n",
    "        tforms.append(tform)\n",
    "        # break\n",
    "\n",
    "for R2_im, source_im, tform in zip(R2_ims, source_ims, tforms):\n",
    "    im_source = img_as_ubyte(io.imread(source_im))\n",
    "    im_target = img_as_ubyte(io.imread(R2_im))\n",
    "\n",
    "    t_target = apply_tform(im_source, im_target, tform.astype(float), multichannel=(len(im_target.shape)==3))\n",
    "\n",
    "    im_name = os.path.basename(source_im)\n",
    "    io.imsave(os.path.join(out_path, im_name), t_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2031, 2556, 3)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im_source.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "17bd8cc65278d68418390df1717e72093edc71987b1c35e4c9075110d61347be"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
